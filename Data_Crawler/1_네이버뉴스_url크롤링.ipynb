{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = requests.get('https://search.naver.com/search.naver?where=news&query=%EC%82%BC%EC%84%B1%EC%A0%84%EC%9E%90&sm=tab_opt&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=2018.01.01&de=2018.01.31&docid=&nso=so%3Ar%2Cp%3Afrom20180101to20180131%2Ca%3Aall&mynews=0&refresh_start=0&related=0')\n",
    "bs =BeautifulSoup(url.text, \"html.parser\")\n",
    "news = bs.select('ul.list_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=001&aid=0009849001\n",
      "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=028&aid=0002397021\n",
      "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=030&aid=0002680023\n",
      "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=015&aid=0003887264\n",
      "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=214&aid=0000802380\n",
      "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=469&aid=0000274979\n",
      "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=056&aid=0010540900\n",
      "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=092&aid=0002130550\n",
      "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=052&aid=0001112764\n",
      "https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=008&aid=0004001179\n"
     ]
    }
   ],
   "source": [
    "for url in bs.select('ul.list_news div.news_info div a'):\n",
    "    if url.attrs['href'].startswith(\"https://news.naver.com\"):\n",
    "        print(url.attrs['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_format = 'https://search.naver.com/search.naver?&where=news&query=삼성전자&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=2019.01.01&de=2019.01.01&docid=&nso=so:r,p:from20190101to20190101,a:all&mynews=0&cluster_rank=34&start=360'\n",
    "url = requests.get(url_format)\n",
    "bs =BeautifulSoup(url.text, \"html.parser\")\n",
    "num = bs.select('div.sc_page_inner')\n",
    "news = bs.select('ul.list_news')\n",
    "num[0].select('a')[-1].attrs['aria-pressed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마지막페이지까지 긁기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "\n",
    "cur_page = 0\n",
    "url_format = 'https://search.naver.com/search.naver?&where=news&query=lg화학&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=2019.01.01&de=2019.01.01&docid=&nso=so:r,p:from20190101to20190101,a:all&mynews=0&cluster_rank=34&start={}'\n",
    "url = url_format.format(cur_page)\n",
    "print(url)\n",
    "url_req = requests.get(url)\n",
    "bs =BeautifulSoup(url_req.text, \"html.parser\")\n",
    "news = bs.select('ul.list_news')\n",
    "num = bs.select('div.sc_page_inner')\n",
    "for url in bs.select('ul.list_news div.news_info div a'):\n",
    "    if url.attrs['href'].startswith(\"https://news.naver.com\"):\n",
    "        print(url.attrs['href'])\n",
    "        urls.append(url.attrs['href'])\n",
    "\n",
    "while True:\n",
    "    cur_page+=10\n",
    "    url = url_format.format(cur_page)\n",
    "    print(url)\n",
    "    url_req = requests.get(url)\n",
    "    bs =BeautifulSoup(url_req.text, \"html.parser\")\n",
    "    news = bs.select('ul.list_news')\n",
    "    num = bs.select('div.sc_page_inner')\n",
    "    for url in bs.select('ul.list_news div.news_info div a'):\n",
    "        if url.attrs['href'].startswith(\"https://news.naver.com\"):\n",
    "            print(url.attrs['href'])\n",
    "            urls.append(url.attrs['href'])\n",
    "    \n",
    "    if num[0].select('a')[-1].attrs['aria-pressed'] == 'true':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 마지막페이지까지 긁은 후에 날짜 넘기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '20180131'\n",
    "end = '20180201'\n",
    "keyword ='삼성전자'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01-31\n",
      "2018-02-01\n"
     ]
    }
   ],
   "source": [
    "start_date = datetime.date(datetime.strptime(start, '%Y%m%d'))\n",
    "end_date = datetime.date(datetime.strptime(end, '%Y%m%d'))\n",
    "print(start_date)\n",
    "print(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://search.naver.com/search.naver?&where=news&query=삼성전자&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds=20180131&de=20180131&docid=&nso=so:r,p:from2018.01.31to2018.01.31,a:all&mynews=0&cluster_rank=34&start=0'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_format = 'https://search.naver.com/search.naver?&where=news&query={0}&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={1}&de={1}&docid=&nso=so:r,p:from{2}to{2},a:all&mynews=0&cluster_rank=34&start={3}'\n",
    "url = url_format.format('삼성전자',datetime.strftime(start_date, '%Y%m%d'),dot_start_date,cur_page)\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '20190101'\n",
    "end = '20191231'\n",
    "keyword ='lg화학'\n",
    "\n",
    "urls_lis = []\n",
    "start_date = datetime.date(datetime.strptime(start, '%Y%m%d'))\n",
    "end_date = datetime.date(datetime.strptime(end, '%Y%m%d'))\n",
    "end_date = datetime.strftime(end_date, '%Y-%m-%d')\n",
    "dot_start_date = datetime.strftime(start_date, '%Y.%m.%d')\n",
    "cur_page = 0\n",
    "url_format = 'https://search.naver.com/search.naver?&where=news&query={0}&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={1}&de={1}&docid=&nso=so:r,p:from{2}to{2},a:all&mynews=0&cluster_rank=34&start={3}'\n",
    "url = url_format.format(keyword,dot_start_date,datetime.strftime(start_date, '%Y%m%d'),cur_page)\n",
    "# print(url)\n",
    "\n",
    "url_req = requests.get(url)\n",
    "bs =BeautifulSoup(url_req.text, \"html.parser\")\n",
    "news = bs.select('ul.list_news')\n",
    "num = bs.select('div.sc_page_inner')\n",
    "for url in bs.select('ul.list_news div.news_info div a'):\n",
    "    if url.attrs['href'].startswith(\"https://news.naver.com\"): # 언론사 홈페이지가 아닌 네이버에서 제공하는 뉴스만 추출\n",
    "        # print(url.attrs['href'])\n",
    "        urls_lis.append(url.attrs['href'])\n",
    "        \n",
    "# 마지막 페이지 나올때까지 다음 페이지 request\n",
    "while True:\n",
    "    cur_page+=10\n",
    "    url = url_format.format(keyword,dot_start_date,datetime.strftime(start_date, '%Y%m%d'),cur_page)\n",
    "    # print(url)\n",
    "    url_req = requests.get(url)\n",
    "    bs =BeautifulSoup(url_req.text, \"html.parser\")\n",
    "    news = bs.select('ul.list_news')\n",
    "    num = bs.select('div.sc_page_inner')\n",
    "    for url in bs.select('ul.list_news div.news_info div a'):\n",
    "        if url.attrs['href'].startswith(\"https://news.naver.com\"):\n",
    "            # print(url.attrs['href'])\n",
    "            urls_lis.append(url.attrs['href'])\n",
    "            \n",
    "    # 마지막 페이지는 aria-pressed 마지막 속성값이 'true'\n",
    "    if num[0].select('a')[-1].attrs['aria-pressed'] == 'true':\n",
    "        break\n",
    "\n",
    "print(start_date)\n",
    "\n",
    "\n",
    "# 다음 날짜 넘기기\n",
    "while True:\n",
    "    start_date+=timedelta(days=1)\n",
    "    cur_page = 0\n",
    "    dot_start_date = datetime.strftime(start_date, '%Y.%m.%d')\n",
    "    url_format = 'https://search.naver.com/search.naver?&where=news&query={0}&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={1}&de={1}&docid=&nso=so:r,p:from{2}to{2},a:all&mynews=0&cluster_rank=34&start={3}'\n",
    "    url = url_format.format(keyword,dot_start_date,datetime.strftime(start_date, '%Y%m%d'),cur_page)\n",
    "    # print(url)\n",
    "    \n",
    "    url_req = requests.get(url)\n",
    "    bs =BeautifulSoup(url_req.text, \"html.parser\")\n",
    "    news = bs.select('ul.list_news')\n",
    "    num = bs.select('div.sc_page_inner')\n",
    "    for url in bs.select('ul.list_news div.news_info div a'):\n",
    "        if url.attrs['href'].startswith(\"https://news.naver.com\"):\n",
    "            # print(url.attrs['href'])\n",
    "            urls_lis.append(url.attrs['href'])\n",
    "        \n",
    "    while True:\n",
    "        cur_page+=10\n",
    "        url = url_format.format(keyword,dot_start_date,datetime.strftime(start_date, '%Y%m%d'),cur_page)\n",
    "        # print(url)\n",
    "        url_req = requests.get(url)\n",
    "        bs =BeautifulSoup(url_req.text, \"html.parser\")\n",
    "        news = bs.select('ul.list_news')\n",
    "        num = bs.select('div.sc_page_inner')\n",
    "        for url in bs.select('ul.list_news div.news_info div a'):\n",
    "            if url.attrs['href'].startswith(\"https://news.naver.com\"):\n",
    "                # print(url.attrs['href'])\n",
    "                urls_lis.append(url.attrs['href'])\n",
    "\n",
    "        if num[0].select('a')[-1].attrs['aria-pressed'] == 'true':\n",
    "            break\n",
    "        \n",
    "    print(start_date)\n",
    "    \n",
    "    # 처음 지정한 end_date와 같으면 다음 페이지 request 중단\n",
    "    if str(start_date) == str(end_date):\n",
    "        print('크롤링 종료')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21873"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(urls_lis)\n",
    "result.to_csv('C:/Users/Inyong Kim/Desktop/lg_2018.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 키워드로 url 긁기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-01\n",
      "2019-01-02\n",
      "2019-01-03\n",
      "2019-01-04\n",
      "2019-01-05\n",
      "2019-01-06\n",
      "2019-01-07\n",
      "2019-01-08\n",
      "2019-01-09\n",
      "2019-01-10\n",
      "2019-01-11\n",
      "2019-01-12\n",
      "2019-01-13\n",
      "2019-01-14\n",
      "2019-01-15\n",
      "2019-01-16\n",
      "2019-01-17\n",
      "2019-01-18\n",
      "2019-01-19\n",
      "2019-01-20\n",
      "2019-01-21\n",
      "2019-01-22\n",
      "2019-01-23\n",
      "2019-01-24\n",
      "2019-01-25\n",
      "2019-01-26\n",
      "2019-01-27\n",
      "2019-01-28\n",
      "2019-01-29\n",
      "2019-01-30\n",
      "2019-01-31\n",
      "2019-02-01\n",
      "2019-02-02\n",
      "2019-02-03\n",
      "2019-02-04\n",
      "2019-02-05\n",
      "2019-02-06\n",
      "2019-02-07\n",
      "2019-02-08\n",
      "2019-02-09\n",
      "2019-02-10\n",
      "2019-02-11\n",
      "2019-02-12\n",
      "2019-02-13\n",
      "2019-02-14\n",
      "2019-02-15\n",
      "2019-02-16\n",
      "2019-02-17\n",
      "2019-02-18\n",
      "2019-02-19\n",
      "2019-02-20\n",
      "2019-02-21\n",
      "2019-02-22\n",
      "2019-02-23\n",
      "2019-02-24\n",
      "2019-02-25\n",
      "2019-02-26\n",
      "2019-02-27\n",
      "2019-02-28\n",
      "2019-03-01\n",
      "2019-03-02\n",
      "2019-03-03\n",
      "2019-03-04\n",
      "2019-03-05\n",
      "2019-03-06\n",
      "2019-03-07\n",
      "2019-03-08\n",
      "2019-03-09\n",
      "2019-03-10\n",
      "2019-03-11\n",
      "2019-03-12\n",
      "2019-03-13\n",
      "2019-03-14\n",
      "2019-03-15\n",
      "2019-03-16\n",
      "2019-03-17\n",
      "2019-03-18\n",
      "2019-03-19\n",
      "2019-03-20\n",
      "2019-03-21\n",
      "2019-03-22\n",
      "2019-03-23\n",
      "2019-03-24\n",
      "2019-03-25\n",
      "2019-03-26\n",
      "2019-03-27\n",
      "2019-03-28\n",
      "2019-03-29\n",
      "2019-03-30\n",
      "2019-03-31\n",
      "2019-04-01\n",
      "2019-04-02\n",
      "2019-04-03\n",
      "2019-04-04\n",
      "2019-04-05\n",
      "2019-04-06\n",
      "2019-04-07\n",
      "2019-04-08\n",
      "2019-04-09\n",
      "2019-04-10\n",
      "2019-04-11\n",
      "2019-04-12\n",
      "2019-04-13\n",
      "2019-04-14\n",
      "2019-04-15\n",
      "2019-04-16\n",
      "2019-04-17\n",
      "2019-04-18\n",
      "2019-04-19\n",
      "2019-04-20\n",
      "2019-04-21\n",
      "2019-04-22\n",
      "2019-04-23\n",
      "2019-04-24\n",
      "2019-04-25\n",
      "2019-04-26\n",
      "2019-04-27\n",
      "2019-04-28\n",
      "2019-04-29\n",
      "2019-04-30\n",
      "2019-05-01\n",
      "2019-05-02\n",
      "2019-05-03\n",
      "2019-05-04\n",
      "2019-05-05\n",
      "2019-05-06\n",
      "2019-05-07\n",
      "2019-05-08\n",
      "2019-05-09\n",
      "2019-05-10\n",
      "2019-05-11\n",
      "2019-05-12\n",
      "2019-05-13\n",
      "2019-05-14\n",
      "2019-05-15\n",
      "2019-05-16\n",
      "2019-05-17\n",
      "2019-05-18\n",
      "2019-05-19\n",
      "2019-05-20\n",
      "2019-05-21\n",
      "2019-05-22\n",
      "2019-05-23\n",
      "2019-05-24\n",
      "2019-05-25\n",
      "2019-05-26\n",
      "2019-05-27\n",
      "2019-05-28\n",
      "2019-05-29\n",
      "2019-05-30\n",
      "2019-05-31\n",
      "2019-06-01\n",
      "2019-06-02\n",
      "2019-06-03\n",
      "2019-06-04\n",
      "2019-06-05\n",
      "2019-06-06\n",
      "2019-06-07\n",
      "2019-06-08\n",
      "2019-06-09\n",
      "2019-06-10\n",
      "2019-06-11\n",
      "2019-06-12\n",
      "2019-06-13\n",
      "2019-06-14\n",
      "2019-06-15\n",
      "2019-06-16\n",
      "2019-06-17\n",
      "2019-06-18\n",
      "2019-06-19\n",
      "2019-06-20\n",
      "2019-06-21\n",
      "2019-06-22\n",
      "2019-06-23\n",
      "2019-06-24\n",
      "2019-06-25\n",
      "2019-06-26\n",
      "2019-06-27\n",
      "2019-06-28\n",
      "2019-06-29\n",
      "2019-06-30\n",
      "크롤링 종료\n"
     ]
    }
   ],
   "source": [
    "start = '20190101'\n",
    "end = '20190630'\n",
    "keyword_1 ='이노베이션'\n",
    "keyword_2 ='소송'\n",
    "\n",
    "urls_lis = []\n",
    "start_date = datetime.date(datetime.strptime(start, '%Y%m%d'))\n",
    "end_date = datetime.date(datetime.strptime(end, '%Y%m%d'))\n",
    "end_date = datetime.strftime(end_date, '%Y-%m-%d')\n",
    "dot_start_date = datetime.strftime(start_date, '%Y.%m.%d')\n",
    "cur_page = 0\n",
    "url_format = 'https://search.naver.com/search.naver?&where=news&query={0}%2C{1}&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={2}&de={2}&docid=&nso=so:r,p:from{3}to{3},a:all&mynews=0&cluster_rank=34&start={4}'\n",
    "url = url_format.format(keyword_1,keyword_2,dot_start_date,datetime.strftime(start_date, '%Y%m%d'),cur_page)\n",
    "# print(url)\n",
    "\n",
    "url_req = requests.get(url)\n",
    "bs =BeautifulSoup(url_req.text, \"html.parser\")\n",
    "news = bs.select('ul.list_news')\n",
    "num = bs.select('div.sc_page_inner')\n",
    "for url in bs.select('ul.list_news div.news_info div a'):\n",
    "    if url.attrs['href'].startswith(\"https://news.naver.com\"): # 언론사 홈페이지가 아닌 네이버에서 제공하는 뉴스만 추출\n",
    "        # print(url.attrs['href'])\n",
    "        urls_lis.append(url.attrs['href'])\n",
    "        \n",
    "# 마지막 페이지 나올때까지 다음 페이지 request\n",
    "try:\n",
    "\n",
    "    while True:\n",
    "        cur_page+=10\n",
    "        url = url_format.format(keyword_1,keyword_2,dot_start_date,datetime.strftime(start_date, '%Y%m%d'),cur_page)\n",
    "        # print(url)\n",
    "        url_req = requests.get(url)\n",
    "        bs =BeautifulSoup(url_req.text, \"html.parser\")\n",
    "        news = bs.select('ul.list_news')\n",
    "        num = bs.select('div.sc_page_inner')\n",
    "        for url in bs.select('ul.list_news div.news_info div a'):\n",
    "            if url.attrs['href'].startswith(\"https://news.naver.com\"):\n",
    "                # print(url.attrs['href'])\n",
    "                urls_lis.append(url.attrs['href'])\n",
    "\n",
    "        # 마지막 페이지는 aria-pressed 마지막 속성값이 'true'\n",
    "        if num[0].select('a')[-1].attrs['aria-pressed'] == 'true':\n",
    "            break\n",
    "\n",
    "except :\n",
    "    pass\n",
    "    \n",
    "print(start_date)\n",
    "\n",
    "\n",
    "# 다음 날짜 넘기기\n",
    "while True:\n",
    "    start_date+=timedelta(days=1)\n",
    "    cur_page = 0\n",
    "    dot_start_date = datetime.strftime(start_date, '%Y.%m.%d')\n",
    "#     url_format = 'https://search.naver.com/search.naver?&where=news&query={0}%2C{1}%2C{2}&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=3&ds={3}&de={3}&docid=&nso=so:r,p:from{4}to{4},a:all&mynews=0&cluster_rank=34&start={5}'\n",
    "    url = url_format.format(keyword_1,keyword_2,dot_start_date,datetime.strftime(start_date, '%Y%m%d'),cur_page)\n",
    "    # print(url)\n",
    "    \n",
    "    url_req = requests.get(url)\n",
    "    bs =BeautifulSoup(url_req.text, \"html.parser\")\n",
    "    news = bs.select('ul.list_news')\n",
    "    num = bs.select('div.sc_page_inner')\n",
    "    for url in bs.select('ul.list_news div.news_info div a'):\n",
    "        if url.attrs['href'].startswith(\"https://news.naver.com\"):\n",
    "            # print(url.attrs['href'])\n",
    "            urls_lis.append(url.attrs['href'])\n",
    "    try:\n",
    "        \n",
    "        while True:\n",
    "            cur_page+=10\n",
    "            url = url_format.format(keyword_1,keyword_2,dot_start_date,datetime.strftime(start_date, '%Y%m%d'),cur_page)\n",
    "            # print(url)\n",
    "            url_req = requests.get(url)\n",
    "            bs =BeautifulSoup(url_req.text, \"html.parser\")\n",
    "            news = bs.select('ul.list_news')\n",
    "            num = bs.select('div.sc_page_inner')\n",
    "            for url in bs.select('ul.list_news div.news_info div a'):\n",
    "                if url.attrs['href'].startswith(\"https://news.naver.com\"):\n",
    "                    # print(url.attrs['href'])\n",
    "                    urls_lis.append(url.attrs['href'])\n",
    "\n",
    "            if num[0].select('a')[-1].attrs['aria-pressed'] == 'true':\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    print(start_date)\n",
    "    \n",
    "    # 처음 지정한 end_date와 같으면 다음 페이지 request 중단\n",
    "    if str(start_date) == str(end_date):\n",
    "        print('크롤링 종료')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(urls_lis)\n",
    "result.to_csv('C:/Users/Inyong Kim/Desktop/innovation_suit.csv', index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
